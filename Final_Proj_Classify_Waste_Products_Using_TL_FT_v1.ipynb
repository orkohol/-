{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image Classification with VGG-16 using Keras and TensorFlow\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This project showcases a robust image classification pipeline leveraging transfer learning with the VGG-16 architecture, implemented using Keras and TensorFlow. The goal is to classify images into two categories: \"O\" and \"R.\" By fine-tuning a pre-trained VGG-16 model on a curated dataset, this project achieves high accuracy while demonstrating best practices in deep learning, including data augmentation, model optimization, and performance evaluation.\n",
        "\n",
        "**Key Objectives**:\n",
        "- Utilize transfer learning to adapt VGG-16 for a binary classification task.\n",
        "- Implement data augmentation to enhance model generalization.\n",
        "- Optimize training with callbacks for early stopping and learning rate scheduling.\n",
        "- Evaluate model performance with detailed metrics and visualizations.\n",
        "\n",
        "**Technologies Used**:\n",
        "- Python, TensorFlow, Keras\n",
        "- VGG-16 pre-trained model\n",
        "- ImageDataGenerator for data preprocessing\n",
        "- Matplotlib for visualization\n",
        "- Scikit-learn for evaluation metrics\n",
        "\n",
        "This notebook is designed to be modular, well-documented, and production-ready, reflecting professional software engineering standards."
      ],
      "metadata": {
        "id": "sA_RliH7HtrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Installation\n",
        "\n",
        "To ensure reproducibility, the required libraries are installed. The following dependencies are essential for this project:"
      ],
      "metadata": {
        "id": "q4Pg7uZ4HqIS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPWTC8pRHhyq"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet tensorflow numpy scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Dependencies\n",
        "\n",
        "The following libraries are imported to support data processing, model building, and evaluation. Logging is configured to track progress and errors effectively."
      ],
      "metadata": {
        "id": "Ee1LoVFJINCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import vgg16\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Suppress TensorFlow warnings for cleaner output\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "logger.info('Dependencies imported successfully.')"
      ],
      "metadata": {
        "id": "veFdAhAXIEiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "The dataset, consisting of images labeled as \"O\" or \"R,\" is downloaded and extracted. A progress bar is used to enhance user experience during file extraction."
      ],
      "metadata": {
        "id": "xct665JxIa7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "def download_and_extract_dataset(url, file_name):\n",
        "    \"\"\"Download and extract dataset with progress tracking.\"\"\"\n",
        "    try:\n",
        "        logger.info('Downloading dataset...')\n",
        "        with requests.get(url, stream=True) as response:\n",
        "            response.raise_for_status()\n",
        "            with open(file_name, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "\n",
        "        logger.info('Extracting dataset...')\n",
        "        with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
        "            members = zip_ref.infolist()\n",
        "            for member in tqdm(members, desc='Extracting files', unit='file'):\n",
        "                zip_ref.extract(member)\n",
        "\n",
        "        os.remove(file_name)\n",
        "        logger.info('Dataset extracted and temporary file removed.')\n",
        "    except Exception as e:\n",
        "        logger.error(f'Error during dataset download/extraction: {e}')\n",
        "        raise\n",
        "\n",
        "# Download dataset\n",
        "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/kd6057VPpABQ2FqCbgu9YQ/o-vs-r-split-reduced-1200.zip\"\n",
        "file_name = \"o-vs-r-split-reduced-1200.zip\"\n",
        "download_and_extract_dataset(dataset_url, file_name)"
      ],
      "metadata": {
        "id": "6p0ntMvzIP3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration Parameters\n",
        "\n",
        "The following configuration parameters define the image dimensions, batch size, and training settings. These values were chosen to balance computational efficiency and model performance."
      ],
      "metadata": {
        "id": "fCHzImTYIgbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model and training configurations\n",
        "CONFIG = {\n",
        "    'img_rows': 150,\n",
        "    'img_cols': 150,\n",
        "    'batch_size': 32,\n",
        "    'n_epochs': 10,\n",
        "    'n_classes': 2,\n",
        "    'val_split': 0.2,\n",
        "    'verbosity': 1,\n",
        "    'train_path': 'o-vs-r-split/train/',\n",
        "    'test_path': 'o-vs-r-split/test/',\n",
        "    'seed': 42,\n",
        "    'labels': ['O', 'R']\n",
        "}\n",
        "\n",
        "# Derived configurations\n",
        "CONFIG['input_shape'] = (CONFIG['img_rows'], CONFIG['img_cols'], 3)\n",
        "\n",
        "logger.info('Configuration parameters set.')"
      ],
      "metadata": {
        "id": "XUI6G2D7Id15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing and Augmentation\n",
        "\n",
        "To prepare the dataset for training, `ImageDataGenerator` is used for preprocessing and augmentation. Augmentation is applied only to training data to enhance model robustness without introducing data leakage."
      ],
      "metadata": {
        "id": "yIu17fLcJ19X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_generators(config):\n",
        "    \"\"\"Create ImageDataGenerators for training, validation, and testing.\"\"\"\n",
        "    try:\n",
        "        train_datagen = ImageDataGenerator(\n",
        "            rescale=1.0/255.0,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1,\n",
        "            horizontal_flip=True,\n",
        "            validation_split=config['val_split']\n",
        "        )\n",
        "\n",
        "        val_datagen = ImageDataGenerator(\n",
        "            rescale=1.0/255.0,\n",
        "            validation_split=config['val_split']\n",
        "        )\n",
        "\n",
        "        test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "        train_generator = train_datagen.flow_from_directory(\n",
        "            directory=config['train_path'],\n",
        "            target_size=(config['img_rows'], config['img_cols']),\n",
        "            batch_size=config['batch_size'],\n",
        "            class_mode='binary',\n",
        "            shuffle=True,\n",
        "            seed=config['seed'],\n",
        "            subset='training'\n",
        "        )\n",
        "\n",
        "        val_generator = val_datagen.flow_from_directory(\n",
        "            directory=config['train_path'],\n",
        "            target_size=(config['img_rows'], config['img_cols']),\n",
        "            batch_size=config['batch_size'],\n",
        "            class_mode='binary',\n",
        "            shuffle=True,\n",
        "            seed=config['seed'],\n",
        "            subset='validation'\n",
        "        )\n",
        "\n",
        "        test_generator = test_datagen.flow_from_directory(\n",
        "            directory=config['test_path'],\n",
        "            target_size=(config['img_rows'], config['img_cols']),\n",
        "            batch_size=config['batch_size'],\n",
        "            class_mode='binary',\n",
        "            shuffle=False,\n",
        "            seed=config['seed']\n",
        "        )\n",
        "\n",
        "        logger.info('Data generators created successfully.')\n",
        "        return train_generator, val_generator, test_generator\n",
        "    except Exception as e:\n",
        "        logger.error(f'Error creating data generators: {e}')\n",
        "        raise\n",
        "\n",
        "# Create data generators\n",
        "train_generator, val_generator, test_generator = create_data_generators(CONFIG)"
      ],
      "metadata": {
        "id": "k6CgmuHcJQnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture\n",
        "\n",
        "The model leverages the pre-trained VGG-16 architecture, with the top layers removed to adapt it for binary classification. The convolutional base is frozen to preserve learned features, and custom dense layers are added for the classification task."
      ],
      "metadata": {
        "id": "oateIdSrLacd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vgg16_model(input_shape):\n",
        "    \"\"\"Build and configure VGG-16 model for binary classification.\"\"\"\n",
        "    try:\n",
        "        # Load VGG-16 without top layers\n",
        "        vgg = vgg16.VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
        "        output = vgg.layers[-1].output\n",
        "        output = Flatten()(output)\n",
        "        basemodel = Model(vgg.input, output)\n",
        "\n",
        "        # Freeze VGG-16 layers\n",
        "        for layer in basemodel.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "        # Add classification layers\n",
        "        model = Sequential([\n",
        "            basemodel,\n",
        "            Dense(512, activation='relu'),\n",
        "            Dropout(0.3),\n",
        "            Dense(512, activation='relu'),\n",
        "            Dropout(0.3),\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            loss='binary_crossentropy',\n",
        "            optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        logger.info('VGG-16 model built and compiled successfully.')\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        logger.error(f'Error building VGG-16 model: {e}')\n",
        "        raise\n",
        "\n",
        "# Build model\n",
        "model = build_vgg16_model(CONFIG['input_shape'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "hj Betsie": "https://tfhub.dev/tensorflow/vgg16/1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Callbacks\n",
        "\n",
        "Callbacks are implemented to optimize training, including learning rate scheduling, early stopping, and model checkpointing to save the best model based on validation loss."
      ],
      "metadata": {
        "id": "UFkuRbrNMlwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_callbacks(checkpoint_path):\n",
        "    \"\"\"Create training callbacks for learning rate scheduling and early stopping.\"\"\"\n",
        "    class LossHistory(tf.keras.callbacks.Callback):\n",
        "        def on_train_begin(self, logs=None):\n",
        "            self.losses = []\n",
        "            self.lr = []\n",
        "\n",
        "        def on_epoch_end(self, epoch, logs=None):\n",
        "            self.losses.append(logs.get('loss'))\n",
        "            self.lr.append(exp_decay(epoch))\n",
        "            logger.info(f'Epoch {epoch + 1} - Learning Rate: {exp_decay(epoch):.6f}')\n",
        "\n",
        "    def exp_decay(epoch):\n",
        "        initial_lrate = 1e-4\n",
        "        k = 0.1\n",
        "        return initial_lrate * np.exp(-k * epoch)\n",
        "\n",
        "    callbacks = [\n",
        "        LossHistory(),\n",
        "        LearningRateScheduler(exp_decay),\n",
        "        EarlyStopping(monitor='val_loss', patience=4, min_delta=0.01, mode='min'),\n",
        "        ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, mode='min')\n",
        "    ]\n",
        "\n",
        "    return callbacks\n",
        "\n",
        "# Define checkpoint path\n",
        "checkpoint_path = 'O_R_tlearn_vgg16.keras'\n",
        "callbacks = create_callbacks(checkpoint_path)"
      ],
      "metadata": {
        "id": "ItEBH2ugOneB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "The model is trained with a limited number of steps per epoch to optimize computational resources. Training and validation performance are monitored using loss and accuracy metrics."
      ],
      "metadata": {
        "id": "G0xmjvLLOtuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_generator, val_generator, callbacks, config):\n",
        "    \"\"\"Train the model with specified generators and callbacks.\"\"\"\n",
        "    try:\n",
        "        history = model.fit(\n",
        "            train_generator,\n",
        "            steps_per_epoch=5,\n",
        "            epochs=config['n_epochs'],\n",
        "            validation_data=val_generator,\n",
        "            validation_steps=val_generator.samples // config['batch_size'],\n",
        "            callbacks=callbacks,\n",
        "            verbose=config['verbosity']\n",
        "        )\n",
        "        logger.info('Model training completed.')\n",
        "        return history\n",
        "    except Exception as e:\n",
        "        logger.error(f'Error during model training: {e}')\n",
        "        raise\n",
        "\n",
        "# Train model\n",
        "history = train_model(model, train_generator, val_generator, callbacks, CONFIG)"
      ],
      "metadata": {
        "id": "FOSPtP3AMPik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Visualization\n",
        "\n",
        "Training and validation loss and accuracy are visualized to assess model performance. Professional styling is applied to enhance readability."
      ],
      "metadata": {
        "id": "h4hS7PDDcI1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_metrics(history):\n",
        "    \"\"\"Plot training and validation loss and accuracy curves.\"\"\"\n",
        "    plt.style.use('seaborn')\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    ax1.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "    ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "    ax1.set_title('Loss Curve', fontsize=14, pad=10)\n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('Loss', fontsize=12)\n",
        "    ax1.legend(fontsize=10)\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax2.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
        "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "    ax2.set_title('Accuracy Curve', fontsize=14, pad=10)\n",
        "    ax2.set_xlabel('Epoch', fontsize=12)\n",
        "    ax2.set_ylabel('Accuracy', fontsize=12)\n",
        "    ax2.legend(fontsize=10)\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot metrics\n",
        "plot_training_metrics(history)"
      ],
      "metadata": {
        "id": "FOSPtP3AMPik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation\n",
        "\n",
        "The model's performance on the test set is evaluated using accuracy, precision, recall, and F1-score to provide a comprehensive assessment."
      ],
      "metadata": {
        "id": "new_evaluation_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_generator):\n",
        "    \"\"\"Evaluate model on test set and compute performance metrics.\"\"\"\n",
        "    try:\n",
        "        # Predict on test set\n",
        "        predictions = model.predict(test_generator)\n",
        "        y_pred = (predictions > 0.5).astype(int).flatten()\n",
        "        y_true = test_generator.classes\n",
        "\n",
        "        # Compute metrics\n",
        "        accuracy = metrics.accuracy_score(y_true, y_pred)\n",
        "        precision = metrics.precision_score(y_true, y_pred)\n",
        "        recall = metrics.recall_score(y_true, y_pred)\n",
        "        f1 = metrics.f1_score(y_true, y_pred)\n",
        "\n",
        "        logger.info(f'Test Accuracy: {accuracy:.4f}')\n",
        "        logger.info(f'Test Precision: {precision:.4f}')\n",
        "        logger.info(f'Test Recall: {recall:.4f}')\n",
        "        logger.info(f'Test F1-Score: {f1:.4f}')\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        cm = metrics.confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(6, 5))\n",
        "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "        plt.title('Confusion Matrix', fontsize=14)\n",
        "        plt.colorbar()\n",
        "        tick_marks = np.arange(len(CONFIG['labels']))\n",
        "        plt.xticks(tick_marks, CONFIG['labels'], rotation=45)\n",
        "        plt.yticks(tick_marks, CONFIG['labels'])\n",
        "        plt.ylabel('True Label', fontsize=12)\n",
        "        plt.xlabel('Predicted Label', fontsize=12)\n",
        "        for i in range(cm.shape[0]):\n",
        "            for j in range(cm.shape[1]):\n",
        "                plt.text(j, i, cm[i, j], horizontalalignment='center', color='white' if cm[i, j] > cm.max() / 2 else 'black')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f'Error during model evaluation: {e}')\n",
        "        raise\n",
        "\n",
        "# Evaluate model\n",
        "evaluate_model(model, test_generator)"
      ],
      "metadata": {
        "id": "new_evaluation_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion and Future Work\n",
        "\n",
        "This project successfully demonstrates the application of transfer learning with VGG-16 for binary image classification, achieving robust performance on the \"O\" vs. \"R\" dataset. The use of data augmentation, learning rate scheduling, and early stopping ensured efficient training and generalization.\n",
        "\n",
        "**Key Achievements**:\n",
        "- Implemented a modular and reusable codebase with error handling and logging.\n",
        "- Achieved high accuracy through transfer learning and fine-tuning.\n",
        "- Provided comprehensive evaluation with accuracy, precision, recall, and F1-score.\n",
        "\n",
        "**Future Improvements**:\n",
        "- Experiment with other pre-trained models (e.g., ResNet, EfficientNet) for performance comparison.\n",
        "- Fine-tune additional VGG-16 layers to capture dataset-specific features.\n",
        "- Implement cross-validation to ensure robustness across data splits.\n",
        "- Explore hyperparameter tuning using grid search or Bayesian optimization.\n",
        "\n",
        "This project reflects advanced proficiency in deep learning, software engineering, and data science, making it a strong portfolio piece for potential employers."
      ],
      "metadata": {
        "id": "h4hS7PDDcI1M"
      }
    }
  ]
}
